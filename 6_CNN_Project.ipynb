{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUyQI6pnBbhF08gcQ4zJt0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arch1781993/Learning_Pytorch_DeepLearning/blob/main/6_CNN_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST dataset"
      ],
      "metadata": {
        "id": "BMY9zVGxXiVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "IKzY-1joU91U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next before we import the data, we need to transform it. the images are 2d, but we need to be working in 4 dimensions,\n",
        "# we need a tensor with 4 dimensions for (to track no of images, height, width and color channel)\n",
        "\n",
        "# Convert MNIST image files into a tensor of 4 dimensions (# of images, height, width and color channel)\n",
        "transform = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "lorLeD1FYkan"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data\n",
        "# MNIST data is already in datasets library, root = 'CNN_data' means we are saving locally on Colab, train = True means it is a training data\n",
        "train_data = datasets.MNIST(root = '/CNN_data', train = True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "XmriVICfaGwj"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test data\n",
        "test_data = datasets.MNIST(root = '/CNN_data', train = False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "iAp750bubfI6"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating small batch size of images for loading, lets say 10\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "X1aM0lhFb3wY"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our model\n",
        "# Describing convulutional layer and what it is doing (2 convulutional layers)\n",
        "# This is just an example, later we will build the whole model\n",
        "conv1 = nn.Conv2d(1,6,3,1)  # this is first convulutional layer: 1 input image, 6 outputs in featured map (with 6 filters), 3*3 kernel and 1 stride\n",
        "                            # Here we have not defined padding (so the images will be reduced by 2 pixels) and we dont care since the digits are in the middle of the images\n",
        "                            # and we dont care what is happening at the edges\n",
        "conv2 = nn.Conv2d(6,16,3,1) # this is second convulutional layer: 6 input image (since there were 6 outputs from 1 con layer),\n",
        "                            # 16 outputs in featured map, 3*3 kernel and 1 stride"
      ],
      "metadata": {
        "id": "c0uGL2wNc5yM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab 1 MNIST image\n",
        "for i, (X_train,y_train) in enumerate(train_data):  # Here X_train is the actual image and y_train is the label associated with this\n",
        "  break"
      ],
      "metadata": {
        "id": "wPamvJvunkn6"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape # 1 iamge and size is 28*28 pixels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtPXtMllopLo",
        "outputId": "b411c467-840b-42a0-a35d-43e959168406"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since we are working with 4d tensor, lets transform this image into 4 dimension\n",
        "x = X_train.view(1,1,28,28)        # 1st batch, 1st image, 28 height, 28 width"
      ],
      "metadata": {
        "id": "SQbuZwBho6Lq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform our first convolution\n",
        "x = F.relu(conv1(x)) # Rectified Linear Unit for our activation function"
      ],
      "metadata": {
        "id": "518VigKTo8wf"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape      # 1st batch, 6 output (since there were 6 filters each 3*3), 26 height and 26 width"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2oUQM7JqUqV",
        "outputId": "ec973319-19d2-4195-ace7-d5fd79b6d82a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass through the pooling layer\n",
        "x = F.max_pool2d(x,2,2) # 2*2 kernel and 2 stride"
      ],
      "metadata": {
        "id": "WCe4M3watVmV"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # 26/2 = 13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpdorFMZvE1H",
        "outputId": "4f5c5c36-75c7-4fe2-8aad-0c8b8a262c94"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 13, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform our second convolution\n",
        "x = F.relu(conv2(x))"
      ],
      "metadata": {
        "id": "Xz3JxGuivHJH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # this time 16 output (why not 6*16, since there are 16 filters?). In 2nd convulution\n",
        "        # each of the 16 filters is not a flat 3*3 square. Instead, each filter is a 3D volume of 3*3*6.\n",
        "        # One filter slides across all 6 input feature maps at the same time. It performs a calculation on all 6 layers and sums them together into one single value."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLqcY7ScwU8s",
        "outputId": "a147597c-8e9f-4504-fb79-469e21be9893"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd pooling layer\n",
        "x = F.max_pool2d(x,2,2)"
      ],
      "metadata": {
        "id": "e75XG1jEwe6o"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # 11/2 = 5.5, round down to 5, because we cant invent data to round up to 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIr24xYjxoBp",
        "outputId": "b02d2c57-2b49-47ce-ff3d-03e7440d3b46"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class convulutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1,6,3,1)\n",
        "    self.conv2 = nn.Conv2d(6,16,3,1)\n",
        "    # Fully connected layer (see figure)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120) # 16*5*5 is the output from 2nd pooling and passing it to 120 neurons\n",
        "    self.fc2 = nn.Linear(120, 84)     # From 120 neurons to 84 neurons\n",
        "    self.fc3 = nn.Linear(84, 10)      # From 84 neurons to 10 neurons\n",
        "\n",
        "  def forward(self,X):\n",
        "    X = F.relu(self.conv1(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "    # Second pass\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "\n",
        "    # Re-view to flatten it out\n",
        "    X = X.view(-1,16*5*5) # -1 so that we can vary the batch size\n",
        "\n",
        "    # Fully connected layers\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "    return F.log_softmax(X, dim=1) # It turns the raw numbers coming out of your last linear layer into logarithmic probabilities. In simple terms, it tells you which digit (0â€“9) the AI thinks it is looking at.\n",
        "                                   # The dim=1 Part, This tells PyTorch which \"direction\" to calculate the probability. Since your data is in batches,\n",
        "                                   # your X looks like a table: Rows are the images, and Columns are the classes (0-9). dim=1 tells the computer:\n",
        "                                   # \"Calculate the probability across the columns (the classes) for each image.\""
      ],
      "metadata": {
        "id": "JQwSyfg7xo5t"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance\n",
        "torch.manual_seed(41)\n",
        "model = convulutionalNetwork()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ4kLmHDEdE2",
        "outputId": "9c6a6f74-46c9-4f86-9877-011253bb489b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "convulutionalNetwork(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001) # Smaller the LR, the longer it gonna take to train"
      ],
      "metadata": {
        "id": "yCKddKxeEz_H"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7IXqYFoZFnW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}